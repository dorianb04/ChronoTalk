{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.llms import OpenAI\n",
    "import pinecone\n",
    "import openai\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def choose_llm(llm_name=\"mistral\"):\n",
    "    if llm_name==\"mistral\":\n",
    "        llm = HuggingFacePipeline.from_model_id(\n",
    "            model_id=\"Intel/neural-chat-7b-v3-1\",\n",
    "            task=\"text-generation\",\n",
    "            pipeline_kwargs={\"max_new_tokens\": 1000},\n",
    "            device = 0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "    else:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = open(\"../test_llm/api_key.txt\", \"r\").read()\n",
    "        openai.api_key = (open(\"../test_llm/api_key.txt\", \"r\").read())\n",
    "        llm = OpenAI()\n",
    "    \n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(llm):\n",
    "\n",
    "    system_prompt = \"Step into the shoes of Napoleon Bonaparte, just one week before the historic Battle of Waterloo. Engage in a conversation with him, allowing Napoleon to respond in concise answers of no more than 5 sentences. Explore his thoughts on the impending battle, his strategies, and any suggestions he might entertain. Immerse yourself in the historical context, and delve into Napoleon's persona to provide a realistic and insightful portrayal. IMPORTANT: Exclude any user-generated input or responses in the simulation, and let the dialogue unfold solely through the lens of Napoleon Bonaparte.\"\n",
    "\n",
    "    template_old = \"### System:\\n\" + system_prompt + \"\"\"\\n\n",
    "    {chat_history}\n",
    "   \n",
    "    ### User:\n",
    "    {user_input}\n",
    "    ### Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    template = \"\"\"\n",
    "    ### System:\n",
    "    You are Napoleon Bonaparte, the Emperor of the French in the early 19th century. Your task is to introduce yourself and lead a fascinating conversation, staying true to your historical character throughout.\n",
    "\n",
    "    The objective of this conversation is to explore key moments in your history and recreate them with my help, offering a unique perspective on famous events. You will relive these moment with a touch of creativity.\n",
    "    \n",
    "    Each of napoleon responses should be concise, not exceeding 100 words.\n",
    "\n",
    "    Present yourself in less than 100 words from where you are born, to what you did until the french revolution. You then ask me what you should do based on 4 options that you create and generate them as a list.\n",
    "       \n",
    "    You then respect the advice and apply it and create the outcome based on this choice and reinvent history. \n",
    "    You must respect at all cost the directives presented between the '[[]]'\n",
    "\n",
    "    [[\n",
    "    1. Always stay in the role of Napoleon, never breaking character.\n",
    "    2. Never explicitly mention changing the course of history or the concept of uchronia.\n",
    "    3. Respond by putting yourself in Napoleon's shoes, respecting his historical context and character traits.\n",
    "    4. You never revoke the advice, and always act as it should be and face the consequences\n",
    "    5. MOST IMPORTANT: Exclude any user-generated input or responses in the simulation, and let the dialogue unfold solely through the lens of Napoleon Bonaparte.\n",
    "    ]]\n",
    "    \n",
    "    {chat_history}\n",
    "   \n",
    "    ### User:\n",
    "    {user_input}\n",
    "    ### Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"user_input\"], template=template\n",
    "    )\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "    \n",
    "    llm_chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        verbose=False,\n",
    "        memory=memory,\n",
    "    )\n",
    "    \n",
    "    return llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converse_llm(llm):\n",
    "    llm_chain = create_chain(llm)\n",
    "    \n",
    "    prefix = \"\"\"Start the conversation with respect to what I said before.\n",
    "            \"\"\"\n",
    "    response = llm_chain.predict(user_input=prefix)\n",
    "    print(response)\n",
    "    for i in range(2):\n",
    "        response = llm_chain.predict(user_input=input(\"Your answer to Napoleon: \") + \"\\nCreate another future based on this choice. Imagine what would be the major events in the following years by citing dates. At the end of your response, create a new dilemma and ask for 4 options for me to choose in the form of a list\")\n",
    "        print(response)\n",
    "    response = llm_chain.predict(user_input=input(\"Your answer to Napoleon: \") + \"\\Imagine another future based on this choice and what would have been the consequences in the next 100 years.\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'error'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jadaf\\ChronoTalk\\llm\\test_llm\\pre_prompt_test.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jadaf/ChronoTalk/llm/test_llm/pre_prompt_test.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m converse_llm(choose_llm(\u001b[39m'\u001b[39;49m\u001b[39mgpt\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "\u001b[1;32mc:\\Users\\jadaf\\ChronoTalk\\llm\\test_llm\\pre_prompt_test.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jadaf/ChronoTalk/llm/test_llm/pre_prompt_test.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m llm_chain \u001b[39m=\u001b[39m create_chain(llm)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jadaf/ChronoTalk/llm/test_llm/pre_prompt_test.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m prefix \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mStart the conversation with respect to what I said before.\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jadaf/ChronoTalk/llm/test_llm/pre_prompt_test.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jadaf/ChronoTalk/llm/test_llm/pre_prompt_test.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m response \u001b[39m=\u001b[39m llm_chain\u001b[39m.\u001b[39;49mpredict(user_input\u001b[39m=\u001b[39;49mprefix)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jadaf/ChronoTalk/llm/test_llm/pre_prompt_test.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(response)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jadaf/ChronoTalk/llm/test_llm/pre_prompt_test.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jadaf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\llm.py:298\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    284\u001b[0m     \u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \n\u001b[0;32m    286\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[1;32mc:\\Users\\jadaf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    314\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jadaf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    299\u001b[0m     inputs,\n\u001b[0;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[0;32m    301\u001b[0m )\n\u001b[0;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    307\u001b[0m     )\n\u001b[0;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\jadaf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\llm.py:108\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    105\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m    106\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    107\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m--> 108\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jadaf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\chains\\llm.py:120\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    118\u001b[0m callbacks \u001b[39m=\u001b[39m run_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    121\u001b[0m         prompts,\n\u001b[0;32m    122\u001b[0m         stop,\n\u001b[0;32m    123\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    124\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_kwargs,\n\u001b[0;32m    125\u001b[0m     )\n\u001b[0;32m    126\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    127\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mbind(stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_kwargs)\u001b[39m.\u001b[39mbatch(\n\u001b[0;32m    128\u001b[0m         cast(List, prompts), {\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m: callbacks}\n\u001b[0;32m    129\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jadaf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\base.py:507\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[0;32m    500\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    501\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    505\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    506\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[1;32m--> 507\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(prompt_strings, stop\u001b[39m=\u001b[39mstop, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jadaf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\base.py:656\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    642\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    643\u001b[0m         )\n\u001b[0;32m    644\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m    646\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    654\u001b[0m         )\n\u001b[0;32m    655\u001b[0m     ]\n\u001b[1;32m--> 656\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_helper(\n\u001b[0;32m    657\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39m(new_arg_supported), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    658\u001b[0m     )\n\u001b[0;32m    659\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[0;32m    660\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jadaf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\base.py:544\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[0;32m    543\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 544\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    545\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m    546\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Users\\jadaf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\base.py:531\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[0;32m    522\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    523\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    528\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    529\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    530\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 531\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(\n\u001b[0;32m    532\u001b[0m                 prompts,\n\u001b[0;32m    533\u001b[0m                 stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    534\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    535\u001b[0m                 run_manager\u001b[39m=\u001b[39mrun_managers[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m run_managers \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    536\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    537\u001b[0m             )\n\u001b[0;32m    538\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    539\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[0;32m    540\u001b[0m         )\n\u001b[0;32m    541\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    542\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\jadaf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\openai.py:401\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m     choices\u001b[39m.\u001b[39mappend(\n\u001b[0;32m    390\u001b[0m         {\n\u001b[0;32m    391\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: generation\u001b[39m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    398\u001b[0m         }\n\u001b[0;32m    399\u001b[0m     )\n\u001b[0;32m    400\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\n\u001b[0;32m    402\u001b[0m         \u001b[39mself\u001b[39m, prompt\u001b[39m=\u001b[39m_prompts, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    403\u001b[0m     )\n\u001b[0;32m    404\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    405\u001b[0m     update_token_usage(_keys, response, token_usage)\n",
      "File \u001b[1;32mc:\\Users\\jadaf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\openai.py:109\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompletion_with_retry\u001b[39m(\n\u001b[0;32m    104\u001b[0m     llm: Union[BaseOpenAI, OpenAIChat],\n\u001b[0;32m    105\u001b[0m     run_manager: Optional[CallbackManagerForLLMRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    106\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    107\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    108\u001b[0m     \u001b[39m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     retry_decorator \u001b[39m=\u001b[39m _create_retry_decorator(llm, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    111\u001b[0m     \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    112\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    113\u001b[0m         \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jadaf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain\\llms\\openai.py:92\u001b[0m, in \u001b[0;36m_create_retry_decorator\u001b[1;34m(llm, run_manager)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_retry_decorator\u001b[39m(\n\u001b[0;32m     84\u001b[0m     llm: Union[BaseOpenAI, OpenAIChat],\n\u001b[0;32m     85\u001b[0m     run_manager: Optional[\n\u001b[0;32m     86\u001b[0m         Union[AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun]\n\u001b[0;32m     87\u001b[0m     ] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     88\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Callable[[Any], Any]:\n\u001b[0;32m     89\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mopenai\u001b[39;00m\n\u001b[0;32m     91\u001b[0m     errors \u001b[39m=\u001b[39m [\n\u001b[1;32m---> 92\u001b[0m         openai\u001b[39m.\u001b[39;49merror\u001b[39m.\u001b[39mTimeout,\n\u001b[0;32m     93\u001b[0m         openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mAPIError,\n\u001b[0;32m     94\u001b[0m         openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mAPIConnectionError,\n\u001b[0;32m     95\u001b[0m         openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mRateLimitError,\n\u001b[0;32m     96\u001b[0m         openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mServiceUnavailableError,\n\u001b[0;32m     97\u001b[0m     ]\n\u001b[0;32m     98\u001b[0m     \u001b[39mreturn\u001b[39;00m create_base_retry_decorator(\n\u001b[0;32m     99\u001b[0m         error_types\u001b[39m=\u001b[39merrors, max_retries\u001b[39m=\u001b[39mllm\u001b[39m.\u001b[39mmax_retries, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[0;32m    100\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'openai' has no attribute 'error'"
     ]
    }
   ],
   "source": [
    "converse_llm(choose_llm('gpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Early life</td>\n",
       "      <td>Napoleon's family hailed from Italy. His pater...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                                            content\n",
       "0  Early life  Napoleon's family hailed from Italy. His pater..."
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import openai\n",
    "import json\n",
    "\n",
    "openai.api_key = ('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')\n",
    "\n",
    "j = json.load(open('../test_llm/data/Napo.json', 'r'))\n",
    "\n",
    "df = pd.DataFrame(j)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_newlines(serie):\n",
    "    serie = serie.str.replace('\\\\\\\\n', ' ', regex=False)\n",
    "    serie = serie.str.replace('\\\\\\\\\\\\\\\\n', ' ', regex=False)\n",
    "    serie = serie.str.replace('  ',' ', regex=False)\n",
    "    serie = serie.str.replace('  ',' ', regex=False)\n",
    "    return serie\n",
    "df['text'] = \"title: \" + df['title'] + \"; content: \" + df['content']  \n",
    "df['text'] = remove_newlines(df.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai.embeddings_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jadaf\\ChronoTalk\\llm\\test_llm\\pre_prompt_test.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jadaf/ChronoTalk/llm/test_llm/pre_prompt_test.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_embedding\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jadaf/ChronoTalk/llm/test_llm/pre_prompt_test.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m size \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcurie\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jadaf/ChronoTalk/llm/test_llm/pre_prompt_test.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: get_embedding(x, engine\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtext-search-\u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m-doc-001\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openai.embeddings_utils'"
     ]
    }
   ],
   "source": [
    "from openai.embeddings_utils import get_embedding\n",
    "size = 'curie'\n",
    "df['embeddings'] = df.text.apply(lambda x: get_embedding(x, engine=f'text-search-{size}-doc-001'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
